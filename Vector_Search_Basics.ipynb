{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee0e82f-95fa-4efa-84e3-9873d4dcdaf1",
   "metadata": {
    "id": "8ee0e82f-95fa-4efa-84e3-9873d4dcdaf1"
   },
   "source": [
    "# Vector Search Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f964d-aa85-4b2f-a04c-71c295fe9d1e",
   "metadata": {
    "id": "2c1f964d-aa85-4b2f-a04c-71c295fe9d1e"
   },
   "source": [
    "# Create the Index\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57b18c-275c-40bf-8a71-bbb0c94b2a7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e57b18c-275c-40bf-8a71-bbb0c94b2a7b",
    "outputId": "70f5716d-a660-4d72-bf09-7f77736bc533",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install openai faiss-cpu pandas jupyter-datatables cassandra-driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d08ec4-0e7b-4819-933f-43ed5ff95e48",
   "metadata": {
    "id": "25d08ec4-0e7b-4819-933f-43ed5ff95e48"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01da99af-da9b-4f38-b841-d802ff23bf2f",
   "metadata": {
    "id": "01da99af-da9b-4f38-b841-d802ff23bf2f"
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra.query import dict_factory\n",
    "from cassandra.query import SimpleStatement\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a93b64-e9e9-41d1-95c9-dc9194a5ec8d",
   "metadata": {
    "id": "b7a93b64-e9e9-41d1-95c9-dc9194a5ec8d"
   },
   "source": [
    "## Keys & Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb1df7-2dcc-4ba6-a941-49c68631bd49",
   "metadata": {
    "id": "ebeb1df7-2dcc-4ba6-a941-49c68631bd49"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Astra DB\n",
    "ASTRA_DB_KEYSPACE = os.environ['ASTRA_DB_KEYSPACE']\n",
    "ASTRA_DB_SECURE_BUNDLE_PATH = os.environ['ASTRA_DB_SECURE_BUNDLE_PATH']\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ['ASTRA_DB_APPLICATION_TOKEN']\n",
    "\n",
    "# OpenAI Token\n",
    "openai_api_key = os.environ['OPENAI_KEY']\n",
    "openai.api_key = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96369f4-d311-44c2-8469-f960a2a8718a",
   "metadata": {
    "id": "a96369f4-d311-44c2-8469-f960a2a8718a"
   },
   "source": [
    "## Select a model to compute embeddings\n",
    "\n",
    "Embeddings are numerical representations of concepts converted to number sequences, which make it easy for computers to understand the relationships between those concepts.\n",
    "\n",
    "This new embedding model from openAI - `text-embedding-ada-002` - replaces five separate models for text search, text similarity, and code search, and outperforms our previous most capable model, Davinci, at most tasks, while being priced 99.8% lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553fece5-8154-4e18-9610-ff4999bfe171",
   "metadata": {
    "id": "553fece5-8154-4e18-9610-ff4999bfe171"
   },
   "outputs": [],
   "source": [
    "model_id = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd4fc1-84a4-4384-bb3e-42ecffab2455",
   "metadata": {
    "id": "bafd4fc1-84a4-4384-bb3e-42ecffab2455"
   },
   "source": [
    "## Connect to Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5b0058-391d-421e-81a0-eb2f7fe684df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c5b0058-391d-421e-81a0-eb2f7fe684df",
    "outputId": "2a69b7bd-bfac-48b3-a8be-68d84349292e"
   },
   "outputs": [],
   "source": [
    "cloud_config= {\n",
    "  'secure_connect_bundle': ASTRA_DB_SECURE_BUNDLE_PATH\n",
    "}\n",
    "auth_provider = PlainTextAuthProvider('token', ASTRA_DB_APPLICATION_TOKEN)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('vector_search_basics')\n",
    "session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670b30f-927f-47da-b71d-0a99092c3f58",
   "metadata": {
    "id": "0670b30f-927f-47da-b71d-0a99092c3f58"
   },
   "source": [
    "## Create Database Schema\n",
    "\n",
    "Note the data type `vector` in the schema below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf0fd4c-8367-4d5f-983f-5aa7d14a8948",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acf0fd4c-8367-4d5f-983f-5aa7d14a8948",
    "outputId": "40cd08f2-dce8-4aff-a20e-969055961d31"
   },
   "outputs": [],
   "source": [
    "# only use this to reset the schema\n",
    "#session.execute(f\"\"\"DROP INDEX IF EXISTS openai_desc\"\"\")\n",
    "#session.execute(f\"\"\"DROP INDEX IF EXISTS minilm_desc\"\"\")\n",
    "#session.execute(f\"\"\"DROP TABLE IF EXISTS products_table\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a941c487-1c6b-4f46-a0a3-305a95931d82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a941c487-1c6b-4f46-a0a3-305a95931d82",
    "outputId": "2f910e29-f369-4ee0-d7c6-196c2c8dc7ed"
   },
   "outputs": [],
   "source": [
    "# # Create Table\n",
    "session.execute(f\"\"\"CREATE TABLE IF NOT EXISTS products_table\n",
    "(product_id int,\n",
    " chunk_id int,\n",
    "\n",
    " product_name text,\n",
    " description text,\n",
    " price text,\n",
    "\n",
    " openai_description_embedding vector<float, 1536>,\n",
    " minilm_description_embedding vector<float, 384>,\n",
    "\n",
    " PRIMARY KEY (product_id, chunk_id))\"\"\")\n",
    "\n",
    "# # Create Index\n",
    "session.execute(f\"\"\"CREATE CUSTOM INDEX IF NOT EXISTS openai_desc ON products_table (openai_description_embedding) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex'\"\"\")\n",
    "session.execute(f\"\"\"CREATE CUSTOM INDEX IF NOT EXISTS minilm_desc ON products_table (minilm_description_embedding) USING 'org.apache.cassandra.index.sai.StorageAttachedIndex'\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe256f-9efb-41f0-8803-d99696c6089b",
   "metadata": {
    "id": "c1fe256f-9efb-41f0-8803-d99696c6089b"
   },
   "source": [
    "## Load the table with data and create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8fd49-90b7-4f8f-b6e8-50a34896e452",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "cec8fd49-90b7-4f8f-b6e8-50a34896e452",
    "outputId": "3b5e3519-fd28-4de1-8b31-c2192236980c"
   },
   "outputs": [],
   "source": [
    "products_list = pd.read_csv('ProductDataset.csv')\n",
    "products_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb4355-9fcd-4795-8406-aee98fd4b11f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "14eb4355-9fcd-4795-8406-aee98fd4b11f",
    "outputId": "6171894b-61a6-4395-8d27-b9603b69a4c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for id, row in products_list.iterrows():\n",
    "  # Create Embedding for each conversation row, save them to the database\n",
    "  text_chunk_length = 2500\n",
    "  text_chunks = [row.description[i:i + text_chunk_length] for i in range(0, len(row.description), text_chunk_length)]\n",
    "  for chunk_id, chunk in enumerate(text_chunks):\n",
    "    pricevalue = row.price if isinstance(row.price, str) else \"\"\n",
    "    full_chunk = f\"{chunk} price: {pricevalue}\"\n",
    "    embedding = openai.Embedding.create(input=full_chunk, model=model_id)['data'][0]['embedding']\n",
    "    query = SimpleStatement(\n",
    "                f\"\"\"\n",
    "                INSERT INTO products_table\n",
    "                (product_id, chunk_id, product_name, description, price, openai_description_embedding)\n",
    "                VALUES (%s, %s, %s, %s, %s, %s)\n",
    "                \"\"\"\n",
    "            )\n",
    "    #display(row)\n",
    "\n",
    "    session.execute(query, (row.product_id, chunk_id, row.product_name, row.description, pricevalue, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc20311-5fde-46b1-b194-4611866f4264",
   "metadata": {
    "id": "2fc20311-5fde-46b1-b194-4611866f4264"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Use the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f833ef-6555-452b-a903-9505c77b75b1",
   "metadata": {
    "id": "83f833ef-6555-452b-a903-9505c77b75b1"
   },
   "source": [
    "In the steps up to this point, we have been creating a schema and loading the table with data, including embeddings we generated through the OpenAI Embedding API.\n",
    "Now we are going to query that table and use the results to give ChatGPT some context to support it's response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ca4e3-7bb6-485b-ac3a-788c1fe3658d",
   "metadata": {
    "id": "466ca4e3-7bb6-485b-ac3a-788c1fe3658d"
   },
   "source": [
    "## Convert a query string into a text embedding to use as part of the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37217051-b389-49eb-8b5b-6efb14d9f8c4",
   "metadata": {
    "id": "37217051-b389-49eb-8b5b-6efb14d9f8c4"
   },
   "source": [
    "This is where the real fun starts.  Provide a question or request to be used as the query.  The source sample database is mostly consumer electronics and appliances, so imagine you're talking to a customer service rep at Best Buy or another electronics store.\n",
    "\n",
    "Here we use the same API that we used to calculate embeddings for each row in the database, but this time we are using your input question to calculate a vector to use in a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891b68c-5e31-4b6f-915f-f05f684529b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "e891b68c-5e31-4b6f-915f-f05f684529b4",
    "outputId": "8d6fe637-9a0f-4de7-d57e-89507ea7bd4b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customer_input = \"What equipement would you recommend for a computer workstation setup costing less than $2000?\"\n",
    "embedding = openai.Embedding.create(input=customer_input, model=model_id)['data'][0]['embedding']\n",
    "display(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93edd66f-4ffc-4133-943e-b0266c704f49",
   "metadata": {
    "id": "93edd66f-4ffc-4133-943e-b0266c704f49"
   },
   "source": [
    "## Find the top 5 results using ANN Similarity\n",
    "\n",
    "Let's take a look at what a query against a vector index could look like.  The query vector has the same dimensions (number of entries in the list) as the embeddings we generated a few steps ago for each row in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be4acfe-bd54-462c-9f96-acae2228d633",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6be4acfe-bd54-462c-9f96-acae2228d633",
    "outputId": "8d8d41a1-89c4-4ec3-80ef-5b9f5414e291"
   },
   "outputs": [],
   "source": [
    "query = SimpleStatement(\n",
    "    f\"\"\"\n",
    "    SELECT *\n",
    "    FROM products_table\n",
    "    ORDER BY openai_description_embedding ANN OF {embedding} LIMIT 5;\n",
    "    \"\"\"\n",
    "    )\n",
    "#display(query)\n",
    "\n",
    "results = session.execute(query)\n",
    "top_5_products = results._current_rows\n",
    "\n",
    "for row in top_5_products:\n",
    "  print(f\"\"\"{row.product_id}, {row.product_name}, {row.description}\\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbffc89a-b9c8-40d5-90b8-f2493eded2d8",
   "metadata": {
    "id": "cbffc89a-b9c8-40d5-90b8-f2493eded2d8"
   },
   "source": [
    "## Ask ChatGPT for some help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d02afb-13b1-4c71-8610-90768e21989e",
   "metadata": {
    "id": "24d02afb-13b1-4c71-8610-90768e21989e"
   },
   "source": [
    "- Here we build a prompt with which we'll query ChatGPT.  Note the \"roles\" in this little conversation give the LLM more context about who that part of the conversation is coming from.\n",
    "- This may take 10-20 seconds to return, so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f517ae3-0f27-418b-93d4-6eb02a781080",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6f517ae3-0f27-418b-93d4-6eb02a781080",
    "outputId": "df707934-a439-4363-ba08-c8f6cad34b31"
   },
   "outputs": [],
   "source": [
    "\n",
    "message_objects = []\n",
    "message_objects.append({\"role\":\"system\",\n",
    "                        \"content\":\"You're a chatbot helping customers with questions and helping them with product recommendations\"})\n",
    "\n",
    "message_objects.append({\"role\":\"user\",\n",
    "                        \"content\": customer_input})\n",
    "\n",
    "message_objects.append({\"role\":\"user\",\n",
    "                        \"content\": \"Please give me a detailed explanation of your recommendations\"})\n",
    "\n",
    "message_objects.append({\"role\":\"user\",\n",
    "                        \"content\": \"Please be friendly and talk to me like a person, don't just give me a list of recommendations\"})\n",
    "\n",
    "message_objects.append({\"role\":\"user\",\n",
    "                        \"content\":\"The computer component itself should be one from the recommended products I will provide\"})\n",
    "\n",
    "message_objects.append({\"role\": \"assistant\",\n",
    "                        \"content\": \"I found these 5 products I would recommend\"})\n",
    "\n",
    "products_list = []\n",
    "\n",
    "for row in top_5_products:\n",
    "    brand_dict = {'role': \"assistant\", \"content\": f\"{row.description}\"}\n",
    "    products_list.append(brand_dict)\n",
    "\n",
    "message_objects.extend(products_list)\n",
    "message_objects.append({\"role\": \"assistant\", \"content\":\"Here's my summarized recommendation of products, and why it would suit you:\"})\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=message_objects\n",
    ")\n",
    "print(completion.choices[0].message['content'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
